{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2743905,"sourceType":"datasetVersion","datasetId":1672910}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Customers Clustering","metadata":{}},{"cell_type":"markdown","source":"**Description**\n\nThis dataset consists of 1 Million+ transaction by over 800K customers for a bank in India. The data contains information such as - customer age (DOB), location, gender, account balance at the time of the transaction, transaction details, transaction amount, etc.\n\n","metadata":{}},{"cell_type":"markdown","source":"**Feature**\n\n- TransactionID (884265): Unite Transaction ID\n- CustomerDOB (day/month/year): Date of Birth\n- CustGender: Gender\n- CustLocation: Location\n- CustAccountBalance\n- TransactionDate\n- TransactionTime: Transaction Time (unix timestamp_\n- TransactionAmount (INR): Amount in INR","metadata":{}},{"cell_type":"markdown","source":"**Target Problem**\n\n1. Perform Clustering / Segmentation on the dataset and identify popular customer groups along with their definitions/rules\n2. Perform Location-wise analysis to identify regional trends in India\n3. Perform transaction-related analysis to identify interesting trends that can be used by a bank to improve / optimi their user experiences\n4. Customer Recency, Frequency, Monetary analysis\n5. Network analysis or Graph analysis of customer data.","metadata":{}},{"cell_type":"markdown","source":"# Library","metadata":{}},{"cell_type":"code","source":"!pip install chardet\n!pip install plotly","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\nfrom pathlib import Path\nimport os\nimport chardet\nimport requests\nimport itertools\nfrom itertools import cycle\nimport pickle\n\nimport time\n\nimport pandas as pd\nimport numpy as np\nfrom scipy import stats","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline\n\n\n# Seaborn\nimport seaborn as sns\nsns.set(style='darkgrid', font_scale=1.4)\n\n\n# Plotly\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\nimport plotly.express as px\n\n#mlxtend\nfrom mlxtend.plotting import scatterplotmatrix\nfrom mlxtend.plotting import heatmap","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import eli5\nfrom eli5.sklearn import PermutationImportance\nfrom sklearn.utils import resample\n\nfrom sklearn.base import BaseEstimator, TransformerMixin, ClassifierMixin, clone\n\n#Feature selection\nfrom sklearn.feature_selection import mutual_info_classif, SelectFromModel, RFE, RFECV, SelectKBest, chi2\n\n#Model selection\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV, RandomizedSearchCV, cross_val_score, cross_validate\n\n#Model pipe\nfrom sklearn.pipeline import Pipeline, _name_estimators\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\n\n#Preprocessing\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder, LabelEncoder, label_binarize\nimport category_encoders as ce\nfrom category_encoders import MEstimateEncoder\nfrom imblearn.over_sampling import SMOTE\n\n#Model\nimport xgboost as xgb\nfrom sklearn.decomposition import PCA\nfrom sklearn.linear_model import LinearRegression, LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.naive_bayes import GaussianNB\n\n#Evalution\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix, roc_curve, auc, precision_recall_curve, average_precision_score\nfrom sklearn.tree import plot_tree \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load Data","metadata":{}},{"cell_type":"code","source":"class LoadingFile():\n    def __init__(self, path, name, url=None):\n        self.path = path\n        self.name = name\n        self.url = url\n\n    def size(self):\n        kib = 1024\n        size = os.path.getsize(Path(self.path))\n        print(f\"{self.name} size: {np.round(size / kib)} Kib\")\n\n    def Encoding_predict(self):\n        file_path = Path(self.path)\n        with open(file_path, 'rb') as f:\n            contents = f.read()\n\n        encoding_info = chardet.detect(contents)\n\n        detected_encoding = encoding_info['encoding']\n        confidence = encoding_info['confidence']\n\n        print(f\"File name: {self.name:<25}\\nEncoding: {detected_encoding:<10}Confidence: {confidence}\")\n\n\n    def download_data(self):\n        data_path = Path(self.path)\n        if data_path.is_dir():\n            print(f\"{data_path} directory exist.\")\n        else:\n            print(f\"Creating {data_path}\")\n            data_path.mkdir(parents=True, exist_ok=True)\n\n        if (data_path / Path(self.name)).exists():\n            print(f\"{self.name} already exist\")\n            pass\n        else:\n            with open(data_path / self.name, \"wb\") as f:\n                req = requests.get(self.url)\n                print(\"Downloading data...\")\n                f.write(req.content)\n                print(\"Done!\")\nname=[]\npath = []\nfor dirname, _, filenames in os.walk('/kaggle/input/bank-customer-segmentation'):\n    for filename in filenames:\n        name.append(filename)\n        path.append(os.path.join(dirname, filename))\n\n        print(f\"Path: {dirname} | Data_Name: {filename}\")\n        size = LoadingFile(os.path.join(dirname, filename), filename).size()\n        print()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}