{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2743905,"sourceType":"datasetVersion","datasetId":1672910}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Customers Clustering","metadata":{}},{"cell_type":"markdown","source":"**Description**\n\nThis dataset consists of 1 Million+ transaction by over 800K customers for a bank in India. The data contains information such as - customer age (DOB), location, gender, account balance at the time of the transaction, transaction details, transaction amount, etc.\n\n","metadata":{}},{"cell_type":"markdown","source":"**Feature**\n\n- TransactionID (884265): Unite Transaction ID\n- CustomerDOB (day/month/year): Date of Birth\n- CustGender: Gender\n- CustLocation: Location\n- CustAccountBalance\n- TransactionDate\n- TransactionTime: Transaction Time (unix timestamp_\n- TransactionAmount (INR): Amount in INR","metadata":{}},{"cell_type":"markdown","source":"**Target Problem**\n\n1. Perform Clustering / Segmentation on the dataset and identify popular customer groups along with their definitions/rules\n2. Perform Location-wise analysis to identify regional trends in India\n3. Perform transaction-related analysis to identify interesting trends that can be used by a bank to improve / optimi their user experiences\n4. Customer Recency, Frequency, Monetary analysis\n5. Network analysis or Graph analysis of customer data.","metadata":{}},{"cell_type":"markdown","source":"# Library","metadata":{}},{"cell_type":"code","source":"!pip install chardet\n!pip install plotly\n!pip install geocoder","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\nfrom pathlib import Path\nimport os\nimport chardet\nimport requests\nimport itertools\nfrom itertools import cycle\nimport pickle\nimport re\n\nimport geocoder\n\nimport time\n\nimport pandas as pd\nimport numpy as np\nfrom scipy import stats","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline\n\n\n# Seaborn\nimport seaborn as sns\nsns.set(style='whitegrid', font_scale=1.4)\n\n\n# Plotly\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\nimport plotly.express as px\n\n#mlxtend\nfrom mlxtend.plotting import scatterplotmatrix\nfrom mlxtend.plotting import heatmap","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import eli5\nfrom eli5.sklearn import PermutationImportance\nfrom sklearn.utils import resample\n\nfrom sklearn.base import BaseEstimator, TransformerMixin, ClassifierMixin, clone\n\n#Feature selection\nfrom sklearn.feature_selection import mutual_info_classif, SelectFromModel, RFE, RFECV, SelectKBest, chi2\n\n#Model selection\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV, RandomizedSearchCV, cross_val_score, cross_validate\n\n#Model pipe\nfrom sklearn.pipeline import Pipeline, _name_estimators\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\n\n#Preprocessing\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder, LabelEncoder, label_binarize\nimport category_encoders as ce\nfrom category_encoders import MEstimateEncoder\nfrom imblearn.over_sampling import SMOTE\n\n#Model\nimport xgboost as xgb\nfrom sklearn.decomposition import PCA\nfrom sklearn.linear_model import LinearRegression, LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.naive_bayes import GaussianNB\n\n#Evalution\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix, roc_curve, auc, precision_recall_curve, average_precision_score\nfrom sklearn.tree import plot_tree \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load Data","metadata":{}},{"cell_type":"code","source":"class LoadingFile():\n    def __init__(self, path, name, url=None):\n        self.path = path\n        self.name = name\n        self.url = url\n\n    def size(self):\n        kib = 1024\n        size = os.path.getsize(Path(self.path))\n        print(f\"{self.name} size: {np.round(size / kib)} Kib\")\n\n    def Encoding_predict(self):\n        file_path = Path(self.path)\n        with open(file_path, 'rb') as f:\n            contents = f.read()\n\n        encoding_info = chardet.detect(contents)\n\n        detected_encoding = encoding_info['encoding']\n        confidence = encoding_info['confidence']\n\n        print(f\"File name: {self.name:<25}\\nEncoding: {detected_encoding:<10}Confidence: {confidence}\")\n\n\n    def download_data(self):\n        data_path = Path(self.path)\n        if data_path.is_dir():\n            print(f\"{data_path} directory exist.\")\n        else:\n            print(f\"Creating {data_path}\")\n            data_path.mkdir(parents=True, exist_ok=True)\n\n        if (data_path / Path(self.name)).exists():\n            print(f\"{self.name} already exist\")\n            pass\n        else:\n            with open(data_path / self.name, \"wb\") as f:\n                req = requests.get(self.url)\n                print(\"Downloading data...\")\n                f.write(req.content)\n                print(\"Done!\")\nname=[]\npath = []\nfor dirname, _, filenames in os.walk('/kaggle/input/bank-customer-segmentation'):\n    for filename in filenames:\n        name.append(filename)\n        path.append(os.path.join(dirname, filename))\n\n        print(f\"Path: {dirname} | Data_Name: {filename}\")\n        size = LoadingFile(os.path.join(dirname, filename), filename).size()\n        print()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load data one for analysis another for analysis\ndf = pd.read_csv(path[0]) #chunksize=1000\nprint(f\"Data Shape : {df.shape}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Check dtypes and memory**","metadata":{}},{"cell_type":"code","source":"df.info(memory_usage=\"deep\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.memory_usage()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*Notes*\n\n- We will need to transform the data to be numeric (int64 or float64) so that we can train machine learning models.\n- If memory not enough, we can do some optimize:\n    - Changing numeric columns to smaller dtype (float16 and int16)\n    - Changing categorical columns to label\n    - Importing data in chunks","metadata":{}},{"cell_type":"markdown","source":"**Missing Data**","metadata":{}},{"cell_type":"code","source":"print('Data MISSING VALUES:')\nprint(df.isna().sum())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*Notes*\n\n- There are 4 features have missing value","metadata":{}},{"cell_type":"markdown","source":"**Duplcate Data**","metadata":{}},{"cell_type":"code","source":"print(f'Duplicates in Data: {df.duplicated().sum()}, ({np.round(100*df.duplicated().sum()/len(df),1)}%)')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*Notes*\n\n- No Duplcate data !!","metadata":{}},{"cell_type":"markdown","source":"**Cardinality of features**","metadata":{}},{"cell_type":"code","source":"df.nunique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*Notes*\n\n- The data may only record 55 TransactionDate","metadata":{}},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"markdown","source":"## Categoric Data","metadata":{}},{"cell_type":"markdown","source":"**TransactionID**","metadata":{}},{"cell_type":"code","source":"df[\"TransactionID\"].shape[0] == df.shape[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*Notes*\n\n- One TransactionID one data","metadata":{}},{"cell_type":"markdown","source":"**CustomerID**","metadata":{}},{"cell_type":"code","source":"ax = sns.countplot(x=df.groupby([\"CustomerID\"]).size().sort_values(ascending=False))\n\nplt.title(\"Number of Customers with Different Transactions Counts\")\nplt.xlabel(\"The number of Transactions\")\nplt.ylabel(\"The number of Customers\")\n# annotate the bars with fmt from matplotlib\nax.bar_label(ax.containers[0], fmt=lambda x: f\"{int(x)}\", size=12)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*Notes*\n\n- There are some people transactions multiple times in this data record.\n\n*Insights*\n\n- Let's check if anyone has missing values and transaction multiple times.","metadata":{}},{"cell_type":"code","source":"mv_customerid = df[df.isna().any(axis=1)][\"CustomerID\"]\n\n# Find transaction more than one time\nmulti_trans_customers = df.groupby(\"CustomerID\").filter(lambda x: len(x) > 1)\n\n# Filter CustomerID, which also has missing value\nmulti_trans_and_mv_cus = multi_trans_customers[multi_trans_customers[\"CustomerID\"].isin(mv_customerid)]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"multi_trans_and_mv_cus.sort_values([\"CustomerID\"]).shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"multi_trans_and_mv_cus.sort_values([\"CustomerID\"]).isna().any()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*Notes*\n\n- Great! We have identified customers with multiple transactions who have missing values. We can fill in the missing values for CustomerDOB, CustGender, and CustLocation (assuming the customer has not changed location) during the missing value filling.\n- For CustAccountBalance, we can update it by subtracting TransactionAmount (INR) from the previous CustAccountBalance.","metadata":{}},{"cell_type":"markdown","source":"**CustomerDOB**","metadata":{}},{"cell_type":"code","source":"# Split DOB to three feature\ndf[\"CustomerYOB\"] = df[\"CustomerDOB\"].str.split(\"/\").map(lambda x: x[2] if type(x) == list else x)\ndf[\"CustomerMOB\"] = df[\"CustomerDOB\"].str.split(\"/\").map(lambda x: x[1] if type(x) == list else x)\ndf[\"CustomerDaOB\"] = df[\"CustomerDOB\"].str.split(\"/\").map(lambda x: x[0] if type(x) == list else x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*Notes*\n\n- Adding new features, CustomerYOB(Year of Birth), CustomerMOB(Month of Birth) and CustomerDOB(Day of Birth)","metadata":{}},{"cell_type":"markdown","source":"Year","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10,6))\nsns.countplot(x = df[\"CustomerYOB\"].dropna().astype(int).sort_values())\nplt.xlabel(\"Year (19XX)\")\nplt.xticks(ticks=range(0, 100, 3), size=8)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*Notes*\n\n- Year has left skew and most of people between 70-97\n- We need find the meaning of 1800","metadata":{}},{"cell_type":"code","source":"sns.countplot(x=df[\"CustomerMOB\"].dropna().astype(int).sort_values())\nplt.xlabel(\"Month\")\nplt.xticks(size=8)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*Notes*\n\n- Lots of people's birthdat are in January ","metadata":{}},{"cell_type":"code","source":"sns.histplot(df[\"CustomerDaOB\"].dropna().astype(int).sort_values())\n\nplt.xticks(ticks=range(1, 32, 2), size=8)\nplt.xlabel(\"Days\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*Notes*\n\n- Because CustomerDOB has lots of 1800/1/1, the month=1 and day=1 has lots of people.\n- Let's see people have this weird date.","metadata":{}},{"cell_type":"code","source":"special_date_customerid = df[df[\"CustomerYOB\"] == \"1800\"][\"CustomerID\"]\n\n# Filter CustomerID, which also has missing value\nmulti_trans_and_special_date_cus = multi_trans_customers[multi_trans_customers[\"CustomerID\"].isin(special_date_customerid)]\nmulti_trans_and_special_date_cus.sort_values(\"CustomerID\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*Notes*\n\n- It seems like the 1/1/1800 is a default for who isn't fill the information. We can replace all 1/1/1800 to the correct birth date.\n- Although same person in the data, but have different CustGender and CustLocation","metadata":{}},{"cell_type":"markdown","source":"**CustGender**","metadata":{}},{"cell_type":"code","source":"ax = sns.countplot(x=df[\"CustGender\"])\nax.bar_label(ax.containers[0], fmt=lambda x: f\"{int(x)}\", size=12)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*Notes*\n\n- Let's see what is T means.\n\n*Insight*\n\n- The reason why M has lot of people is that many people has 1/1/1800 also is gender M","metadata":{}},{"cell_type":"code","source":"t_cusid = df[df[\"CustGender\"] == \"T\"][\"CustomerID\"].values\ndf[df[\"CustomerID\"].isin(t_cusid)]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*Notes*\n\n- The customer who Gender=T only have one trasaction data.","metadata":{}},{"cell_type":"code","source":"df[df[\"CustomerYOB\"] == \"1800\"][\"CustGender\"].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*Notes*\n\n- It seems like Gender=M is the default value of CustGender. We will try our best fill gender to correct.","metadata":{}},{"cell_type":"markdown","source":"**CustLocation**","metadata":{}},{"cell_type":"code","source":"df[\"CustLocation\"].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*Notes*\n\n- It seems location has different granuity.\n- We can create new feature log and alt.","metadata":{}},{"cell_type":"code","source":"df[df[\"CustLocation\"] == \".\"].shape[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*Notes*\n\n- There are 86 location name ., so we will treat it as missing value.","metadata":{}},{"cell_type":"code","source":"df[\"CustLocation\"] = df[\"CustLocation\"].replace(\".\", np.nan)\ndf[df[\"CustLocation\"] == \".\"].shape[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Search longitude latitude and add to data frame","metadata":{}},{"cell_type":"code","source":"def change_to_lat_lng(val):\n    g = geocoder.arcgis(val)\n    return g.latlng\n\nlat_lon = df[\"CustLocation\"].map(lambda x : change_to_lat_lng(x) if type(x) == str else np.nan)\n\nlats = []\nlons = []\n\nfor lat, lon in lat_lon.values:\n    lats.append(lat)\n    lons.append(lon)\ndf[\"Lon\"] = lons","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Saving dataframe because it take too much time for transforming location to lon and lat\ndf.to_csv(\"df.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**TransactionDate**","metadata":{}},{"cell_type":"code","source":"df[\"TransactionDate\"] = df[\"TransactionDate\"].map(lambda x : f'20{x.split(\"/\")[2]}/{x.split(\"/\")[1]}/{x.split(\"/\")[0]}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split DOB to three feature\ndf[\"TransactionY\"] = df[\"TransactionDate\"].str.split(\"/\").map(lambda x: x[0] if type(x) == list else x)\ndf[\"TransactionM\"] = df[\"TransactionDate\"].str.split(\"/\").map(lambda x: x[1] if type(x) == list else x)\ndf[\"TransactionD\"] = df[\"TransactionDate\"].str.split(\"/\").map(lambda x: x[2] if type(x) == list else x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Transaction Year","metadata":{}},{"cell_type":"code","source":"df[\"TransactionY\"].unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*Notes*\n\n- Only 16 in year of transaction data","metadata":{}},{"cell_type":"markdown","source":"Transaction Month","metadata":{}},{"cell_type":"code","source":"ax = sns.countplot(x=df[\"TransactionM\"].dropna().astype(int).sort_values())\nax.bar_label(ax.containers[0], fmt=lambda x: f\"{int(x)}\", size=12)\nplt.xlabel(\"Month\")\nplt.xticks(size=8)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Transaction Day","metadata":{}},{"cell_type":"code","source":"sns.histplot(df[\"TransactionD\"].dropna().astype(int).sort_values())\n\nplt.xticks(ticks=range(1, 32, 2), size=8)\nplt.xlabel(\"Days\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Weekday","metadata":{}},{"cell_type":"code","source":"df[\"Transaction_weekday\"] = df[\"TransactionDate\"].map(lambda x : \n                                                      pd.Timestamp(int(x.split(\"/\")[0]),\n                                                                   int(x.split(\"/\")[1]),\n                                                                   int(x.split(\"/\")[2])).weekday()) + 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ax = sns.countplot(x=df[\"Transaction_weekday\"].dropna().astype(int).sort_values())\nax.bar_label(ax.containers[0], fmt=lambda x: f\"{int(x)}\", size=12)\nplt.xlabel(\"Weekday\")\nplt.xticks(size=8)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*Notes*\n\n- Most of people transaction on Sunday.","metadata":{}},{"cell_type":"markdown","source":"## Numeric Data","metadata":{}},{"cell_type":"markdown","source":"**CustAccountBalance**","metadata":{}},{"cell_type":"markdown","source":"We average same customer's balance or diff to merge data.","metadata":{}},{"cell_type":"code","source":"avg_cus_balance = df.groupby(\"CustomerID\")[\"CustAccountBalance\"].agg(np.mean)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.boxplot(avg_cus_balance.values)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*Notes*\n\n- Their are lots of large values, which means some customer has higher savings but most have lower savings.","metadata":{}},{"cell_type":"markdown","source":"**TransactionTime**","metadata":{}},{"cell_type":"code","source":"# Transform to india time zone\ndf[\"TransactionTime\"] = df[\"TransactionTime\"].map(lambda x : pd.Timestamp(x, unit='s', tz='Asia/Kolkata')).dt.strftime('%H:%M:%S')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"Transaction_hour\"] = df[\"TransactionTime\"].map(lambda x : x.split(\":\")[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(x=df[\"Transaction_hour\"].sort_values())\nplt.xticks(size=8)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*Notes*\n\n- Most people transacte at 1:00 PM.","metadata":{}},{"cell_type":"markdown","source":"**TransactionAmount (INR)**","metadata":{}},{"cell_type":"code","source":"avg_cus_tans = df.groupby(\"CustomerID\")[\"CustAccountBalance\"].agg(np.mean)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.boxplot(avg_cus_tans.values)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*Notes*\n\n- It seems like transaction amount is like balance, both have a lot outliers. This mean some people have high transaction durning this time.","metadata":{}},{"cell_type":"markdown","source":"# Missing value","metadata":{}},{"cell_type":"markdown","source":"**CustomerDOB**","metadata":{}},{"cell_type":"code","source":"df[\"CustomerDOB\"] = df[\"CustomerDOB\"].replace(\"1/1/1800\", np.nan)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Fill by mutiple times' transaction customer","metadata":{}},{"cell_type":"code","source":"def fill_missing_values(group):\n    return group.sort_values().fillna(method=\"ffill\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"b_filling = np.sum(df[\"CustomerDOB\"].isna())\nfilled_dob = df.groupby(\"CustomerID\")[\"CustomerDOB\"].transform(fill_missing_values)\ndf[\"CustomerDOB\"] = filled_dob","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Before filling: {b_filling}\\nAfter filling: {np.sum(filled_dob.isna())}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**CustGender**","metadata":{}},{"cell_type":"code","source":"b_filling = np.sum(df[\"CustGender\"].isna())\nfilled_gender = df.groupby(\"CustomerID\")[\"CustGender\"].transform(fill_missing_values)\ndf[\"CustGender\"] = filled_gender","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Before filling: {b_filling}\\nAfter filling: {np.sum(filled_gender.isna())}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**CustLocation**","metadata":{}},{"cell_type":"code","source":"b_filling = np.sum(df[\"CustLocation\"].isna())\nfilled_loc = df.groupby(\"CustomerID\")[\"CustLocation\"].transform(fill_missing_values)\ndf[\"CustLocation\"] = filled_loc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Before filling: {b_filling}\\nAfter filling: {np.sum(filled_loc.isna())}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**CustAccountBalance**","metadata":{}},{"cell_type":"code","source":"b_filling = np.sum(df[\"CustAccountBalance\"].isna())\nfilled_bal = df.groupby(\"CustomerID\")[\"CustAccountBalance\"].transform(fill_missing_values)\ndf[\"CustAccountBalance\"] = filled_bal","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Before filling: {b_filling}\\nAfter filling: {np.sum(filled_bal.isna())}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Enginner","metadata":{}},{"cell_type":"markdown","source":"**Age**","metadata":{}},{"cell_type":"code","source":"df[\"Customerage\"] = df[\"CustomerDOB\"].map(lambda x: 100 - int(x.split(\"/\")[2]) + 16 + 1 if type(x) == str else x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Trans_proportion**","metadata":{}},{"cell_type":"code","source":"df[\"trans_proportion\"] = df[\"TransactionAmount (INR)\"] / (df[\"CustAccountBalance\"] + df[\"TransactionAmount (INR)\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.histplot(df[\"trans_proportion\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*Notes*\n\n- Proportion close 1.0 means they trans most of savings in one transaction.","metadata":{}},{"cell_type":"markdown","source":"Let's group same customer wiht mean.","metadata":{}},{"cell_type":"code","source":"sns.histplot(df.groupby(\"CustomerID\")[\"trans_proportion\"].agg(np.mean))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*Notes*\n\n- The proportion 0.3-0.5 are not appear in the histplot which we don't group. This means some people have large diffent transaction amount in different times.","metadata":{}},{"cell_type":"markdown","source":"**Multiple times transaction**","metadata":{}},{"cell_type":"code","source":"cusid_trans_times = df.groupby([\"CustomerID\"]).size()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"transaction_times\"] = df[\"CustomerID\"].map(lambda x : cusid_trans_times[x])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature For Clustering","metadata":{}},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cluster_features = []","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"markdown","source":"Clustering Models:\n\n- Linear Clustering:\n    1. K-mean\n    2. Hierarchical\n    3. PCA\n    4. LDA\n- non-Linear Clustering:\n    5. t-SNE\n    6. DBSCAN\n    7. UMAP","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}