{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":120043040,"sourceType":"kernelVersion"}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Library","metadata":{}},{"cell_type":"code","source":"# Numerical Operations\nimport math\nimport numpy as np\n\n# Reading/Writing Data\nimport pandas as pd\nimport os\nimport csv\n\n# For Progress Bar\nfrom tqdm import tqdm\n\n# Pytorch\nimport torch \nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader, random_split\n\n# For plotting learning curve\nfrom torch.utils.tensorboard import SummaryWriter","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Helper Function**","metadata":{}},{"cell_type":"code","source":"def same_seed(seed): \n    '''Fixes random number generator seeds for reproducibility.'''\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class COVID19Dataset(Dataset):\n    '''\n    x: Features.\n    y: Targets, if none, do prediction.\n    '''\n    def __init__(self, x, y=None):\n        if y is None:\n            self.y = y\n        else:\n            self.y = torch.FloatTensor(y)\n        self.x = torch.FloatTensor(x)\n\n    def __getitem__(self, idx):\n        if self.y is None:\n            return self.x[idx]\n        else:\n            return self.x[idx], self.y[idx]\n\n    def __len__(self):\n        return len(self.x)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"class My_Model(nn.Module):\n    def __init__(self, input_dim):\n        super(My_Model, self).__init__()\n        # TODO: modify model's structure, be aware of dimensions. \n        self.layers = nn.Sequential(\n            nn.Linear(input_dim, 16),\n            nn.ReLU(),\n            nn.Linear(16, 8),\n            nn.ReLU(),\n            nn.Linear(8, 1)\n        )\n\n    def forward(self, x):\n        x = self.layers(x)\n        x = x.squeeze(1) # (B, 1) -> (B)\n        return x","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature Selection\n","metadata":{}},{"cell_type":"code","source":"def select_feat(train_data, valid_data, test_data, select_all=True):\n    '''Selects useful features to perform regression'''\n    y_train, y_valid = train_data[:,-1], valid_data[:,-1]\n    raw_x_train, raw_x_valid, raw_x_test = train_data[:,:-1], valid_data[:,:-1], test_data\n\n    if select_all:\n        feat_idx = list(range(raw_x_train.shape[1]))\n    else:\n        feat_idx = list(range(35, raw_x_train.shape[1])) # TODO: Select suitable feature columns.\n        \n    return raw_x_train[:,feat_idx], raw_x_valid[:,feat_idx], raw_x_test[:,feat_idx], y_train, y_valid","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Config","metadata":{}},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'\nconfig = {\n    'seed': 5201314,      # Your seed number, you can pick your lucky number. :)\n    'select_all': True,   # Whether to use all features.\n    'valid_ratio': 0.2,   # validation_size = train_size * valid_ratio\n    'n_epochs': 5000,     # Number of epochs.            \n    'batch_size': 256, \n    'learning_rate': 1e-5,              \n    'early_stop': 600,    # If model has not improved for this many consecutive epochs, stop training.     \n    'save_path': './models/model.ckpt' # Your model will be saved here.\n}","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dataloader","metadata":{}},{"cell_type":"code","source":"# Set seed for reproducibility\nsame_seed(config['seed'])\n\n\n# train_data size: 3009 x 89 (35 states + 18 features x 3 days) \n# test_data size: 997 x 88 (without last day's positive rate)\ntrain_data, test_data = pd.read_csv('./covid_train.csv').values, pd.read_csv('./covid_test.csv').values\ntrain_data, valid_data = train_valid_split(train_data, config['valid_ratio'], config['seed'])\n\n# Print out the data size.\nprint(f\"\"\"train_data size: {train_data.shape} \nvalid_data size: {valid_data.shape} \ntest_data size: {test_data.shape}\"\"\")\n\n# Select features\nx_train, x_valid, x_test, y_train, y_valid = select_feat(train_data, valid_data, test_data, config['select_all'])\n\n# Print out the number of features.\nprint(f'number of features: {x_train.shape[1]}')\n\ntrain_dataset, valid_dataset, test_dataset = COVID19Dataset(x_train, y_train), \\\n                                            COVID19Dataset(x_valid, y_valid), \\\n                                            COVID19Dataset(x_test)\n\n# Pytorch data loader loads pytorch dataset into batches.\ntrain_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True, pin_memory=True)\nvalid_loader = DataLoader(valid_dataset, batch_size=config['batch_size'], shuffle=True, pin_memory=True)\ntest_loader = DataLoader(test_dataset, batch_size=config['batch_size'], shuffle=False, pin_memory=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training Model","metadata":{}},{"cell_type":"code","source":"# create model, define a loss function, and optimizer\nmodel = My_Model(input_dim=x_train.shape[1]).to(device)\ncriterion = nn.MSELoss(reduction='mean')\noptimizer = torch.optim.SGD(model.parameters(), lr=config['learning_rate'], momentum=0.7) \n#optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The L2 regularized loss is L = f(Î¸) + Â½Î»âˆ‘Î¸Â². But then, ðœ•L/ðœ•Î¸ = ðœ•f/ðœ•Î¸ + Î»âˆ‘Î¸. If we take a look at the Adam algorithm, it effectively says g = ðœ•L/ðœ•Î¸ = ðœ•f/ðœ•Î¸ + Î»âˆ‘Î¸.","metadata":{}},{"cell_type":"code","source":"def train(model: torch.nn.Module,\n          data_loader: torch.utils.data.DataLoader,\n          loss_fn: torch.nn.Module,\n          optimizer: torch.optim.Optimizer,\n          device: torch.device):\n    model.train()\n    train_acc = 0.0\n    train_loss = 0.0\n    for features, labels in tqdm(data_loader):\n        features = features.squeeze()\n        labels = labels.squeeze()\n        features, labels = features.to(device), labels.to(device)\n        # 1. Froward pass\n        outputs = model(features)\n\n         # 2. Calculate loss\n        loss = loss_fn(outputs, labels)\n\n        # 3. Optimizer zer grade\n        optimizer.zero_grad()\n\n        # 4. Loss backward\n        loss.backward()\n\n        # 5. optimizer step\n        optimizer.step()\n\n        _, train_pred = torch.max(outputs, 1)\n        train_acc += (train_pred == labels).sum().item()/labels.size(0)\n        train_loss += loss.item()/labels.size(0)\n\n    return train_acc / len(data_loader.dataset), train_loss / len(data_loader) #data_loader.dataset total sample size and data_loader batch size\n\n\ndef validate(model: torch.nn.Module,\n             data_loader: torch.utils.data.DataLoader,\n             loss_fn: torch.nn.Module,\n             device: torch.device):\n    model.eval()\n    val_acc = 0.0\n    val_loss = 0.0\n    with torch.no_grad():\n        for features, labels in tqdm(data_loader):\n            features = features.squeeze()\n            labels = labels.squeeze()\n            features, labels = features.to(device), labels.to(device)\n\n            outputs = model(features)\n            loss = loss_fn(outputs, labels)\n\n            _, val_pred = torch.max(outputs, 1)\n            val_acc += (val_pred == labels).sum().item()/labels.size(0)\n            val_loss += loss.item()/labels.size(0)\n\n    return val_acc / len(data_loader.dataset), val_loss / len(data_loader)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for epoch in range(num_epoch):\n    train_acc, train_loss = train(model, train_loader, criterion, optimizer, device)\n    val_acc, val_loss = validate(model, val_loader, criterion, device)\n    train_info[\"Acc\"].append(train_acc)\n    train_info[\"Loss\"].append(train_loss)\n    val_info[\"Acc\"].append(val_acc)\n    val_info[\"Loss\"].append(val_loss)\n    print(f'[{epoch+1:03d}/{num_epoch:03d}] Train Acc: {train_acc:.5f} Loss: {train_loss:.5f} | Val Acc: {val_acc:.5f} Loss: {val_loss:.5f}')\n\n    if val_acc > best_acc:\n        best_acc = val_acc\n        torch.save(model.state_dict(), model_path)\n        print(f'Saving model with acc {best_acc:.5f}')\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def test(model: torch.nn.Module,\n             data_loader: torch.utils.data.DataLoader,\n             loss_fn: torch.nn.Module,\n             device: torch.device):\n    test_pred=[]\n    model.eval()\n    with torch.no_grad():\n        for features in tqdm(data_loader):\n            features = features.squeeze()\n            features = features.to(device)\n            outputs = model(features)\n\n            _, val_pred = torch.max(outputs, 1)\n            test_pred.append(val_pred)\n\n    return test_pred\n\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_pred = test(model, test_loader, criterion, device)\n\nwith open(\"pred_v1.txt\", \"w\") as fh:\n    for idx, line in enumerate(open(\"/kaggle/input/libraphone/libriphone/test_split.txt\").readlines()):\n        fh.write(line.rstrip()+\" \")\n        for p in test_pred[idx].tolist():\n            fh.write(str(p)+\" \")\n        fh.write(\"\\n\")","metadata":{},"execution_count":null,"outputs":[]}]}