{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":48952,"databundleVersionId":5186467,"sourceType":"competition"}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Library","metadata":{}},{"cell_type":"code","source":"!nvidia-smi","metadata":{"execution":{"iopub.status.busy":"2024-07-26T01:58:33.281391Z","iopub.execute_input":"2024-07-26T01:58:33.282082Z","iopub.status.idle":"2024-07-26T01:58:34.439355Z","shell.execute_reply.started":"2024-07-26T01:58:33.282050Z","shell.execute_reply":"2024-07-26T01:58:34.438436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import random\nimport os\nimport argparse\nfrom tqdm import tqdm\nfrom pathlib import Path\nimport numpy as np\nimport cv2\n\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\nfrom torch.utils.data import Dataset\nimport torch.nn as nn\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms, datasets\nimport gc\nimport torchvision.transforms.functional as F\nfrom torch.utils.tensorboard import SummaryWriter\n\nfrom timeit import default_timer as timer\n\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns","metadata":{"execution":{"iopub.status.busy":"2024-07-26T03:49:43.970706Z","iopub.execute_input":"2024-07-26T03:49:43.971137Z","iopub.status.idle":"2024-07-26T03:50:05.066442Z","shell.execute_reply.started":"2024-07-26T03:49:43.971103Z","shell.execute_reply":"2024-07-26T03:50:05.065097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Helper Function","metadata":{}},{"cell_type":"code","source":"def same_seeds(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.benchmarkbe = False\n    torch.backends.cudnn.deterministic = True","metadata":{"execution":{"iopub.status.busy":"2024-07-26T03:50:05.068755Z","iopub.execute_input":"2024-07-26T03:50:05.069544Z","iopub.status.idle":"2024-07-26T03:50:05.076772Z","shell.execute_reply.started":"2024-07-26T03:50:05.069507Z","shell.execute_reply":"2024-07-26T03:50:05.075196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_transformed_images(image_paths, transform=None, n=3): \n    \n    random_image_paths = random.sample(image_paths, k=n) \n    for i, image_path in enumerate(random_image_paths):\n        fig = plt.figure(figsize=(14,120))\n        with Image.open(image_path) as img:\n            ax = fig.add_subplot(n, 4, i*4+1)\n            ax.imshow(img)\n\n            if i == 0:\n                ax.set_title('Orig.', size=15)\n\n            ax = fig.add_subplot(n, 4, i*4+2)\n            img_transform = transforms.Compose([\n                transforms.RandomCrop([178, 178])\n                ])\n            img_cropped = img_transform(img)\n            ax.imshow(img_cropped)\n\n            if i == 0:\n                ax.set_title('Step 1: Random crop', size=15) \n\n            ax = fig.add_subplot(n, 4, i*4+3)\n            img_transform = transforms.Compose([\n                transforms.RandomHorizontalFlip()\n                ])\n            img_flip = img_transform(img_cropped)\n            ax.imshow(img_flip)\n\n            if i == 0:\n                ax.set_title('Step 2: Random flip', size=15)\n\n            ax = fig.add_subplot(n, 4, i*4+4)\n            img_resized = transforms.functional.resize(\n                img_flip, size=(128, 128)\n                )\n            img_flip = img_transform(img_cropped)\n            ax.imshow(img_resized)\n\n            if i == 0:\n                ax.set_title('step 3: Reszie', size=15)\n\n            if i == 2 :\n                break\n","metadata":{"execution":{"iopub.status.busy":"2024-07-26T03:54:14.365566Z","iopub.execute_input":"2024-07-26T03:54:14.366330Z","iopub.status.idle":"2024-07-26T03:54:14.377390Z","shell.execute_reply.started":"2024-07-26T03:54:14.366296Z","shell.execute_reply":"2024-07-26T03:54:14.376035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load Data","metadata":{}},{"cell_type":"code","source":"data_path = Path(\"/kaggle/input/ml2023spring-hw3\")\ntrain_path = data_path/ \"train\"\nvalid_path = data_path/ \"valid\"\ntest_path = data_path/ \"test\"","metadata":{"execution":{"iopub.status.busy":"2024-07-26T03:50:56.648654Z","iopub.execute_input":"2024-07-26T03:50:56.649071Z","iopub.status.idle":"2024-07-26T03:50:56.654698Z","shell.execute_reply.started":"2024-07-26T03:50:56.649038Z","shell.execute_reply":"2024-07-26T03:50:56.653537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def walk_through_dir(dir_path):\n    for dirpath, dirnames, filenames in os.walk(dir_path):\n        print(dirpath)\n        file_len = len([filename for filename in filenames if filename.split('.')[1] == 'jpg'])\n        print(f\"There are {len(dirnames)} directories and {file_len} images and {len(filenames)-file_len} others\")\n        print()\n              \nwalk_through_dir(data_path)","metadata":{"execution":{"iopub.status.busy":"2024-07-26T03:50:57.499142Z","iopub.execute_input":"2024-07-26T03:50:57.499573Z","iopub.status.idle":"2024-07-26T03:51:20.076702Z","shell.execute_reply.started":"2024-07-26T03:50:57.499541Z","shell.execute_reply":"2024-07-26T03:51:20.075429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"same_seeds(42)\n\nimage_path_list = list(train_path.glob(\"*.jpg\"))\nrandom_image_path = random.choice(image_path_list)\nimg = Image.open(random_image_path)\n\nprint(f\"Path: {random_image_path}\")\nprint(f\"Class: {str(random_image_path).split('/')[-1].split('_')[0]}\")\nprint(f\"Height: {img.height}\")\nprint(f\"Width: {img.width}\")\nimg","metadata":{"execution":{"iopub.status.busy":"2024-07-26T03:51:20.078648Z","iopub.execute_input":"2024-07-26T03:51:20.079017Z","iopub.status.idle":"2024-07-26T03:51:20.304911Z","shell.execute_reply.started":"2024-07-26T03:51:20.078989Z","shell.execute_reply":"2024-07-26T03:51:20.303906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"height = 128\nwidth = 128\n\ndata_transform = transforms.Compose([\n    \n    #transforms.RandomCrop([356, 356]),\n    \n    transforms.RandomHorizontalFlip(),\n    \n    transforms.RandomRotation(30),\n    \n    transforms.RandomResizedCrop(size=256, scale=(0.8, 1.2)),\n    \n    #transforms.RandomCrop(size=32, padding=4),\n    \n    transforms.ColorJitter(brightness=0.2, contrast=0.1, saturation=0, hue=0),\n    \n    # Flip the images randomly on the horizontal \n    transforms.RandomHorizontalFlip(p=0.5),\n    \n    # Resize the images \n    transforms.Resize(size=(height, width)),\n    \n \n       \n    # Turn the image into a torch.Tensor\n    transforms.ToTensor(), # this also converts all pixel values from 0 to \n    \n    # Cutout(n_holes=1, length=16),\n])\n\nplot_transformed_images(image_path_list, n=5)","metadata":{"execution":{"iopub.status.busy":"2024-07-26T03:54:17.423682Z","iopub.execute_input":"2024-07-26T03:54:17.424574Z","iopub.status.idle":"2024-07-26T03:54:20.958206Z","shell.execute_reply.started":"2024-07-26T03:54:17.424537Z","shell.execute_reply":"2024-07-26T03:54:20.956977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def print_train_time(start:float,\n                     end:float,\n                     device:torch.device =None):\n    total_time = end - start\n    print(f\"Train time on {device}:{total_time:.3f} seconds\")\n    return total_time\n","metadata":{"execution":{"iopub.status.busy":"2024-07-25T11:00:51.382924Z","iopub.execute_input":"2024-07-25T11:00:51.383273Z","iopub.status.idle":"2024-07-25T11:00:51.389451Z","shell.execute_reply.started":"2024-07-25T11:00:51.383244Z","shell.execute_reply":"2024-07-25T11:00:51.388363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Hyper-parameters","metadata":{}},{"cell_type":"code","source":"# training parameters\nseed=42\nbatch_size = 32                # batch size\nnum_epoch = 50                   # the number of training epoch\nlearning_rate = 1e-3 \nweight_decay=1e-5 # learning rate\nmodel_path = './model_1.ckpt'     # the path where the checkpoint will be saved\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'","metadata":{"execution":{"iopub.status.busy":"2024-07-26T04:00:08.448118Z","iopub.execute_input":"2024-07-26T04:00:08.448629Z","iopub.status.idle":"2024-07-26T04:00:08.455586Z","shell.execute_reply.started":"2024-07-26T04:00:08.448591Z","shell.execute_reply":"2024-07-26T04:00:08.454011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading to Dataset","metadata":{}},{"cell_type":"code","source":"#train_data = datasets.ImageFolder(root = train_path, transform=data_transform, target_transform=None)\n#valid_data = datasets.ImageFolder(root = valid_path, transform=data_transform, target_transform=None)\n#test_data = datasets.ImageFolder(root = test_path, transform=data_transform)\n\n#print(f\"Train data:\\n{train_data}\\nValid data:\\n{valid_data}\\nTest data:\\n{test_data}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-25T11:00:51.406701Z","iopub.execute_input":"2024-07-25T11:00:51.407053Z","iopub.status.idle":"2024-07-25T11:00:51.411890Z","shell.execute_reply.started":"2024-07-25T11:00:51.407015Z","shell.execute_reply":"2024-07-25T11:00:51.410857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ImageDataset(Dataset):\n    def __init__(self, root_path, transform=None, label=None):\n        self.label = label\n        self.data_paths = []\n        if label:\n            self.label_paths = []\n        \n        for f in os.listdir(root_path):\n            self.data_paths.append(str(root_path)+\"/\"+f)\n            if label:\n                self.label_paths.append(float(f.split(\"_\")[0]))\n                \n        self.transform = transform\n\n    def __getitem__(self, idx):\n        with Image.open(self.data_paths[idx]) as f:\n            if self.label:\n                y = torch.LongTensor([self.label_paths[idx]])\n            if self.transform:\n                X = self.transform(f)\n            else:\n                X = torch.from_numpy(np.array(f))\n\n            if self.label:\n                return X, y\n            return X\n\n    def __len__(self):\n        return len(self.data_paths)\n    \n","metadata":{"execution":{"iopub.status.busy":"2024-07-26T04:00:30.797265Z","iopub.execute_input":"2024-07-26T04:00:30.797732Z","iopub.status.idle":"2024-07-26T04:00:30.807190Z","shell.execute_reply.started":"2024-07-26T04:00:30.797698Z","shell.execute_reply":"2024-07-26T04:00:30.805948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Normalize**","metadata":{}},{"cell_type":"code","source":"train_transform = transforms.Compose([\n    \n    # Resize the images \n    transforms.Resize(size=(height, width)),\n    \n    # Turn the image into a torch.Tensor\n    transforms.ToTensor() # this also converts all pixel values from 0 to \n])","metadata":{"execution":{"iopub.status.busy":"2024-07-26T04:00:32.235279Z","iopub.execute_input":"2024-07-26T04:00:32.236109Z","iopub.status.idle":"2024-07-26T04:00:32.242136Z","shell.execute_reply.started":"2024-07-26T04:00:32.236068Z","shell.execute_reply":"2024-07-26T04:00:32.240672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = ImageDataset(root_path=train_path, transform=train_transform, label=True)","metadata":{"execution":{"iopub.status.busy":"2024-07-26T04:00:33.139602Z","iopub.execute_input":"2024-07-26T04:00:33.140018Z","iopub.status.idle":"2024-07-26T04:00:33.165636Z","shell.execute_reply.started":"2024-07-26T04:00:33.139987Z","shell.execute_reply":"2024-07-26T04:00:33.164185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2024-07-26T04:00:33.378571Z","iopub.execute_input":"2024-07-26T04:00:33.378979Z","iopub.status.idle":"2024-07-26T04:00:33.384696Z","shell.execute_reply.started":"2024-07-26T04:00:33.378948Z","shell.execute_reply":"2024-07-26T04:00:33.383440Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mean = 0.\nstd = 0.\nnb_samples = 0.\n\nfor data in train_loader:\n    images, _ = data\n    batch_samples = images.size(0)\n    images = images.view(batch_samples, images.size(1), -1) # (batch_size, channels, height, width) to (batch_size, channels, height * width)\n    mean += images.mean(2).sum(0) # images.mean(2) calculate to (batch_size, channels)\n    std += images.std(2).sum(0)\n    nb_samples += batch_samples\n\nmean /= nb_samples\nstd /= nb_samples\n\nprint(f'Mean: {mean}')\nprint(f'Std: {std}')","metadata":{"execution":{"iopub.status.busy":"2024-07-26T04:00:34.384191Z","iopub.execute_input":"2024-07-26T04:00:34.384935Z","iopub.status.idle":"2024-07-26T04:02:50.553488Z","shell.execute_reply.started":"2024-07-26T04:00:34.384890Z","shell.execute_reply":"2024-07-26T04:02:50.552112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Train Transform**","metadata":{}},{"cell_type":"code","source":"train_transform = transforms.Compose([\n    \n    #transforms.RandomCrop([178, 178]),\n    \n    # Flip the images randomly on the horizontal \n    transforms.RandomHorizontalFlip(p=0.5), \n     \n    # Resize the images \n    transforms.Resize(size=(height, width)),\n\n    # Turn the image into a torch.Tensor\n    transforms.ToTensor(), # this also converts all pixel values from 0 to \n        \n    transforms.Normalize(mean=mean.tolist(), std=std.tolist()),\n    \n])","metadata":{"execution":{"iopub.status.busy":"2024-07-26T04:02:50.555711Z","iopub.execute_input":"2024-07-26T04:02:50.556127Z","iopub.status.idle":"2024-07-26T04:02:50.562606Z","shell.execute_reply.started":"2024-07-26T04:02:50.556096Z","shell.execute_reply":"2024-07-26T04:02:50.561483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Test Transform**","metadata":{}},{"cell_type":"code","source":"test_transform = transforms.Compose([\n    \n    #transforms.CenterCrop([178, 178]),\n    \n    # Resize the images \n    transforms.Resize(size=(height, width)),\n    \n    # Turn the image into a torch.Tensor\n    transforms.ToTensor(), # this also converts all pixel values from 0 to \n    \n     transforms.Normalize(mean=mean.tolist(), std=std.tolist()),\n])","metadata":{"execution":{"iopub.status.busy":"2024-07-26T04:02:50.564160Z","iopub.execute_input":"2024-07-26T04:02:50.564559Z","iopub.status.idle":"2024-07-26T04:02:50.579296Z","shell.execute_reply.started":"2024-07-26T04:02:50.564524Z","shell.execute_reply":"2024-07-26T04:02:50.578122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Data Loader Prepare**","metadata":{}},{"cell_type":"code","source":"train_data = ImageDataset(root_path=train_path, transform=train_transform, label=True)\nvalid_data = ImageDataset(root_path=valid_path, transform=test_transform, label=True)\ntest_data = ImageDataset(root_path=test_path, transform=test_transform)","metadata":{"execution":{"iopub.status.busy":"2024-07-26T04:02:50.582068Z","iopub.execute_input":"2024-07-26T04:02:50.582450Z","iopub.status.idle":"2024-07-26T04:02:50.616401Z","shell.execute_reply.started":"2024-07-26T04:02:50.582410Z","shell.execute_reply":"2024-07-26T04:02:50.614968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\nvalid_loader = DataLoader(valid_data, batch_size=batch_size, shuffle=False)\ntest_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-07-26T04:02:50.617630Z","iopub.execute_input":"2024-07-26T04:02:50.618009Z","iopub.status.idle":"2024-07-26T04:02:50.624564Z","shell.execute_reply.started":"2024-07-26T04:02:50.617977Z","shell.execute_reply":"2024-07-26T04:02:50.623317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Dataloaders: {train_loader, valid_loader, test_loader}\")\nprint(f\"Length of train dataloader: {len(train_loader)} batches of {batch_size}\")\nprint(f\"Length of valid dataloader: {len(valid_loader)} batches of {batch_size}\")\nprint(f\"Length of test dataloader: {len(test_loader)} batches of {batch_size}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-26T04:02:50.625845Z","iopub.execute_input":"2024-07-26T04:02:50.626170Z","iopub.status.idle":"2024-07-26T04:02:50.638621Z","shell.execute_reply.started":"2024-07-26T04:02:50.626142Z","shell.execute_reply":"2024-07-26T04:02:50.637417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modeling","metadata":{}},{"cell_type":"markdown","source":"**Model Contruction**","metadata":{}},{"cell_type":"markdown","source":"`nn.Flatten()` turn [C, H, W] to [C, H x W]","metadata":{}},{"cell_type":"code","source":"class IMAGEModelV0(nn.Module):\n    def __init__(self, X_height: int, X_width: int, fc_out: int):\n        super(IMAGEModelV0, self).__init__()\n\n        self.features = nn.Sequential(\n            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=1, stride=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2),\n            nn.Dropout(p=0.5),\n\n            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1, stride=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2),\n            nn.Dropout(p=0.5),\n\n            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1, stride=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2),\n\n            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1, stride=1),\n            nn.BatchNorm2d(512),\n            nn.ReLU(),\n            nn.AvgPool2d(kernel_size=2),\n            \n            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1, stride=1),\n            nn.BatchNorm2d(512),\n            nn.ReLU(),\n            nn.AvgPool2d(kernel_size=2)\n        )\n\n        self.flat = nn.Flatten(start_dim=1, end_dim=-1)\n\n        # 計算每層卷積和池化後的輸出大小\n        conv1_output_size = self.cnn_output_size(X_height, 1, 3) // 2\n        conv2_output_size = self.cnn_output_size(conv1_output_size, 1, 3) // 2\n        conv3_output_size = self.cnn_output_size(conv2_output_size, 1, 3) // 2\n        conv4_output_size = self.cnn_output_size(conv3_output_size, 1, 3) // 2\n        conv5_output_size = self.cnn_output_size(conv4_output_size, 1, 3) // 2\n\n        self.classifier = nn.Sequential(\n            nn.Linear(in_features=conv5_output_size * conv5_output_size * 512, out_features=1024),\n            nn.ReLU(),\n            nn.Dropout(p=0.5),\n            nn.Linear(in_features=1024, out_features=512),\n            nn.Dropout(p=0.5),\n            nn.ReLU(),\n            nn.Linear(in_features=512, out_features=fc_out),\n        )\n\n    def forward(self, x):\n        x = self.features(x)\n        x = self.flat(x)\n        x = self.classifier(x)\n        return x\n\n    def cnn_output_size(self, n, p, m, s=1):\n        return (n + 2 * p - m) // s + 1\n\nmodel = IMAGEModelV0(X_height=height,\n                     X_width=width,\n                     fc_out=11).to(device)","metadata":{"execution":{"iopub.status.busy":"2024-07-26T04:00:16.723875Z","iopub.execute_input":"2024-07-26T04:00:16.724283Z","iopub.status.idle":"2024-07-26T04:00:16.892238Z","shell.execute_reply.started":"2024-07-26T04:00:16.724251Z","shell.execute_reply":"2024-07-26T04:00:16.891195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model","metadata":{"execution":{"iopub.status.busy":"2024-07-26T04:00:17.475327Z","iopub.execute_input":"2024-07-26T04:00:17.475769Z","iopub.status.idle":"2024-07-26T04:00:17.483567Z","shell.execute_reply.started":"2024-07-26T04:00:17.475735Z","shell.execute_reply":"2024-07-26T04:00:17.482385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**loss_fn and optimizer**","metadata":{}},{"cell_type":"code","source":"loss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)","metadata":{"execution":{"iopub.status.busy":"2024-07-26T04:00:18.881325Z","iopub.execute_input":"2024-07-26T04:00:18.881726Z","iopub.status.idle":"2024-07-26T04:00:18.887750Z","shell.execute_reply.started":"2024-07-26T04:00:18.881698Z","shell.execute_reply":"2024-07-26T04:00:18.886535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Training & Validation**","metadata":{}},{"cell_type":"code","source":"def train(model, num_epochs, train_dl, valid_dl, loss_fn, optimizer, seed, device):\n    train_time_start = timer()\n    same_seeds(seed)\n    loss_hist_train = [0] * num_epochs\n    accuracy_hist_train = [0] * num_epochs\n    \n    loss_hist_valid = [0] * num_epochs\n    accuracy_hist_valid = [0] * num_epochs\n    \n    best_acc = 0.0\n    \n    for epoch in range(num_epochs):\n        model.train()\n        for x_batch, y_batch in tqdm(train_dl):\n            x_batch, y_batch = x_batch.to(device), y_batch.squeeze(1).to(device)\n            pred = model(x_batch)\n            \n            loss = loss_fn(pred, y_batch)\n            \n            loss.backward()\n            \n            optimizer.step()\n            \n            optimizer.zero_grad()\n            \n            loss_hist_train[epoch] += loss\n            is_correct = (torch.argmax(pred, dim=1) == y_batch).float()\n            accuracy_hist_train[epoch] += is_correct.sum()\n            \n        loss_hist_train[epoch] /= len(train_dl.dataset)\n        accuracy_hist_train[epoch] /= len(train_dl.dataset)\n        \n        writer.add_scalar(\"Loss/train\", loss_hist_train[epoch], epoch)\n        writer.add_scalar(\"Accuracy/train\", accuracy_hist_train[epoch], epoch)\n        \n        model.eval()\n        with torch.no_grad():\n            for x_batch, y_batch in tqdm(valid_dl):\n                x_batch, y_batch = x_batch.to(device), y_batch.squeeze(1).to(device)\n                pred = model(x_batch)\n                loss = loss_fn(pred, y_batch)\n                \n                loss_hist_valid[epoch] += loss\n                is_correct = (torch.argmax(pred, dim=1) == y_batch).float()\n                accuracy_hist_valid[epoch] += is_correct.sum()\n                \n        loss_hist_valid[epoch] /= len(valid_dl.dataset)\n        accuracy_hist_valid[epoch] /= len(valid_dl.dataset)\n        writer.add_scalar(\"Loss/test\", loss_hist_valid[epoch], epoch)\n        writer.add_scalar(\"Accuracy/test\", accuracy_hist_valid[epoch], epoch)\n        \n        \n        print(f\"Epoch {epoch+1}/{num_epochs}\")\n        print(\"-\"*10)\n        print(f\"Train_loss: {loss_hist_train[epoch]:.4f} | Train_accuracy: {accuracy_hist_train[epoch]*100:.4f}%\")\n        print(f\"val_loss: {loss_hist_valid[epoch]:.4f} | val_accuracy: {accuracy_hist_valid[epoch]*100:.4f}%\\n\")\n        \n        if accuracy_hist_valid[epoch] > best_acc:\n            best_acc = accuracy_hist_valid[epoch]\n            torch.save(model.state_dict(), model_path)\n            print(f'Saving model with acc {best_acc:.5f}')\n    \n    train_time_end = timer()\n    total_train_time_model_1 = print_train_time(start=train_time_start, end=train_time_end, device=device)\n    \n    return loss_hist_train, loss_hist_valid, accuracy_hist_train, accuracy_hist_valid","metadata":{"execution":{"iopub.status.busy":"2024-07-26T04:00:20.696175Z","iopub.execute_input":"2024-07-26T04:00:20.697585Z","iopub.status.idle":"2024-07-26T04:00:20.712343Z","shell.execute_reply.started":"2024-07-26T04:00:20.697542Z","shell.execute_reply":"2024-07-26T04:00:20.711074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"writer = SummaryWriter(\"./run\")\nloss_hist_train, loss_hist_valid, accuracy_hist_train, accuracy_hist_valid = train(model=model,\n                                                                                      num_epochs=num_epoch,\n                                                                                      train_dl=train_loader,\n                                                                                      valid_dl=valid_loader,\n                                                                                      loss_fn=loss_fn,\n                                                                                      optimizer=optimizer,\n                                                                                     seed=seed, device=device)\nwriter.flush()","metadata":{"execution":{"iopub.status.busy":"2024-07-26T04:02:50.640069Z","iopub.execute_input":"2024-07-26T04:02:50.640515Z","iopub.status.idle":"2024-07-26T04:02:59.233551Z","shell.execute_reply.started":"2024-07-26T04:02:50.640475Z","shell.execute_reply":"2024-07-26T04:02:59.231957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_arr = np.arange(0, num_epochs+1)\nfig = plt.figure(figsize=(12, 4))\n\nax = fig.add_subplot(1, 2, 1)\nax.plot(x_arr, loss_hist_train, '-o', label='Train loss')\nax.plot(x_arr, loss_hist_valid, '--<', label='Validation loss')\nax.legend(fontsize=15)\n\nax = fig.add_subplot(1, 2, 2)\nax.plot(x_arr, accuracy_hist_train, '-o', label='Train acc.')\nax.plot(x_arr, accuracy_hist_valid, '--<',label='Validation acc.')\nax.legend(fontsize=15)\nax.set_xlabel('Epoch', size=15)\nax.set_ylabel('Accuracy', size=15)\n\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"state_dict = torch.load(\"/kaggle/working/model_1.ckpt\")\nmodel.load_state_dict(state_dict)","metadata":{"execution":{"iopub.status.busy":"2024-07-25T02:38:32.199806Z","iopub.execute_input":"2024-07-25T02:38:32.200190Z","iopub.status.idle":"2024-07-25T02:38:32.252137Z","shell.execute_reply.started":"2024-07-25T02:38:32.200153Z","shell.execute_reply":"2024-07-25T02:38:32.251186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Evalution**","metadata":{}},{"cell_type":"code","source":"def inference(model, dl, device):\n    same_seeds(seed)\n    loss = 0\n    accuracy = 0\n    result = []\n    actual = []\n    \n    model.eval()\n    with torch.no_grad():\n        for x_batch, y_batch in dl:\n            x_batch, y_batch = x_batch.to(device), y_batch.squeeze(1).to(device)\n            pred = model(x_batch)\n            result.extend(pred.argmax(axis=1).cpu().tolist())\n            actual.extend(y_batch.cpu().tolist())\n            loss = loss_fn(pred, y_batch)\n\n            loss += loss.item()\n            is_correct = (torch.argmax(pred, dim=1) == y_batch).float()\n            accuracy += is_correct.sum()\n\n    loss /= len(dl.dataset)\n    accuracy /= len(dl.dataset)    \n\n\n    print(f\"test_loss: {loss:.4f} | test_accuracy: {accuracy*100:.4f}%\")\n\n    return result, actual","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred, actual = inference(model=aug_model, dl=valid_loader, device=device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_confusion_matrix(pred, actual):\n        cm = confusion_matrix(pred, actual)\n        cm_norm = confusion_matrix(pred, actual, normalize=\"true\")\n\n        annot = np.array([f\"{i}\\n({g:.1%})\" for i, g in zip(cm.flatten(), cm_norm.flatten())])\n        annot = annot.reshape(cm.shape)\n\n        fig = plt.figure(figsize=(8,8),dpi=90)\n        sns.heatmap(cm, annot=annot, fmt=\"\", xticklabels=range(0,12), yticklabels=range(0,12), cmap='Blues')\n        plt.title(\"Confusion Matrix\")\n        plt.xlabel('Predicted')\n        plt.ylabel('Actual')\n        plt.show()\n        \nplot_confusion_matrix(pred, actual)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*Notes*\n\n- label 7 is hard to predict\n- label 2 has low accuracy may cause by few data to train.","metadata":{}},{"cell_type":"code","source":"def visualize_random_samples(model, val_dl, device, class_names, num_images=8):\n    model.eval()\n    transform = transforms.ToPILImage()\n    \n    fig, axes = plt.subplots(nrows=(num_images + 3) // 4, ncols=4, figsize=(15, 15))\n    axes = axes.flatten()\n    \n    with torch.no_grad():\n        for i in range(num_images):\n            # 隨機選擇一個批次和一個圖片\n            batch_idx = random.randint(0, len(val_dl)-1)\n            for idx, (x_batch, y_batch) in enumerate(val_dl):\n                if idx == batch_idx:\n                    img_idx = random.randint(0, x_batch.size(0)-1)\n                    img, label = x_batch[img_idx], y_batch[img_idx]\n                    break\n            \n            # 將圖片轉移到指定裝置\n            img = img.to(device).unsqueeze(0)\n            label = label.to(device).unsqueeze(0)\n            \n            # 模型預測\n            pred = model(img)\n            prob = torch.nn.functional.softmax(pred, dim=1)[0]\n            pred_label = torch.argmax(prob).item()\n            actual_label = label.item()\n            \n            # 取得對應的類別名稱\n            pred_class = class_names[pred_label]\n            actual_class = class_names[actual_label]\n            prob_value = prob[pred_label].item()\n            \n            # 將圖片從tensor轉換為numpy陣列，然後顯示圖片\n            img = img.cpu().squeeze(0)\n            img = transform(img)\n            \n            ax = axes[i]\n            ax.imshow(np.array(img))\n            ax.set_title(f'Pred: {pred_class} ({prob_value*100:.2f}%)\\nActual: {actual_class}')\n            ax.axis('off')\n            \n            # 設定外框顏色\n            if pred_label == actual_label:\n                color = 'green'\n            else:\n                color = 'red'\n            for spine in ax.spines.values():\n                spine.set_edgecolor(color)\n                spine.set_linewidth(2)\n\n    plt.tight_layout()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-26T04:19:42.863367Z","iopub.execute_input":"2024-07-26T04:19:42.864317Z","iopub.status.idle":"2024-07-26T04:19:42.876171Z","shell.execute_reply.started":"2024-07-26T04:19:42.864277Z","shell.execute_reply":"2024-07-26T04:19:42.875079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"visualize_random_samples(model, valid_loader, device, np.arange(0,12), num_images=8)","metadata":{"execution":{"iopub.status.busy":"2024-07-26T04:20:32.489506Z","iopub.execute_input":"2024-07-26T04:20:32.490572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install tensorboard\ntensorboard --logdir=runs","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Augmentation","metadata":{}},{"cell_type":"code","source":"class Cutout(object):\n    def __init__(self, n_holes, length):\n        self.n_holes = n_holes\n        self.length = length\n\n    def __call__(self, img):\n        h = img.size(1)\n        w = img.size(2)\n\n        mask = np.ones((h, w), np.float32)\n\n        for _ in range(self.n_holes):\n            y = np.random.randint(h)\n            x = np.random.randint(w)\n\n            y1 = np.clip(y - self.length // 2, 0, h)\n            y2 = np.clip(y + self.length // 2, 0, h)\n            x1 = np.clip(x - self.length // 2, 0, w)\n            x2 = np.clip(x + self.length // 2, 0, w)\n\n            mask[y1: y2, x1: x2] = 0.\n\n        mask = torch.from_numpy(mask)\n        mask = mask.expand_as(img)\n        img = img * mask\n\n        return img","metadata":{"execution":{"iopub.status.busy":"2024-07-25T11:03:08.314822Z","iopub.execute_input":"2024-07-25T11:03:08.315238Z","iopub.status.idle":"2024-07-25T11:03:08.324917Z","shell.execute_reply.started":"2024-07-25T11:03:08.315207Z","shell.execute_reply":"2024-07-25T11:03:08.323671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class AugmentImageDataset(Dataset):\n    def __init__(self, root_path, transform=None, label=None):\n        self.label = label\n        self.data_paths = []\n        if label:\n            self.label_paths = []\n        \n        for f in os.listdir(root_path):\n            self.data_paths.append(os.path.join(root_path, f))\n            if label:\n                self.label_paths.append(float(f.split(\"_\")[0]))\n                \n        self.transform = transform\n\n    def __getitem__(self, idx):\n        with Image.open(self.data_paths[idx]) as f:\n            if self.label:\n                y = torch.LongTensor([self.label_paths[idx]])\n            transformed_images = []    \n            if isinstance(self.transform, list):\n                for transform in self.transform:\n                    if transform.__class__.__name__ == \"Cutout\":\n                        trans = transforms.Compose([transforms.ToTensor()])  \n                        transformed_image = trans(f)\n                        transformed_image = transform(transformed_image)\n                    else:\n                        trans = transforms.Compose([transform, transforms.ToTensor()])\n                        transformed_image = trans(f)\n                        \n                    resize = transforms.Compose([transforms.Resize(size=(height, width)),\n                                                 transforms.Normalize(mean=mean.tolist(), std=std.tolist())])    \n                    transformed_images.append(resize(transformed_image))\n                \n                nothing_trans = transforms.Compose([transforms.ToTensor(), \n                                                    transforms.Resize(size=(height, width)),\n                                                    transforms.Normalize(mean=mean.tolist(), std=std.tolist())]) \n                transformed_images.append(nothing_trans(f))\n            else:\n                transformed_images.append(self.transform(f))\n\n            if self.label:\n                return torch.stack(transformed_images), y\n            return torch.stack(transformed_images)\n\n    def __len__(self):\n        return len(self.data_paths)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-07-25T11:03:08.514471Z","iopub.execute_input":"2024-07-25T11:03:08.514919Z","iopub.status.idle":"2024-07-25T11:03:08.545980Z","shell.execute_reply.started":"2024-07-25T11:03:08.514876Z","shell.execute_reply":"2024-07-25T11:03:08.544992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transform_list = [\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(30),\n    #transforms.RandomResizedCrop(size=256, scale=(0.8, 1.2)),\n    transforms.RandomCrop(size=128, padding=4),\n    transforms.ColorJitter(brightness=0.2, contrast=0.1, saturation=0, hue=0),\n    Cutout(n_holes=1, length=16),\n]\n","metadata":{"execution":{"iopub.status.busy":"2024-07-25T11:03:10.111431Z","iopub.execute_input":"2024-07-25T11:03:10.111778Z","iopub.status.idle":"2024-07-25T11:03:10.117688Z","shell.execute_reply.started":"2024-07-25T11:03:10.111752Z","shell.execute_reply":"2024-07-25T11:03:10.116706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = AugmentImageDataset(root_path=train_path, transform=transform_list, label=True)\nvalid_data = ImageDataset(root_path=valid_path, transform=test_transform, label=True)","metadata":{"execution":{"iopub.status.busy":"2024-07-25T11:03:10.613027Z","iopub.execute_input":"2024-07-25T11:03:10.613424Z","iopub.status.idle":"2024-07-25T11:03:10.661842Z","shell.execute_reply.started":"2024-07-25T11:03:10.613394Z","shell.execute_reply":"2024-07-25T11:03:10.661017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"aug_batch_size = 16\n\ntrain_loader = DataLoader(train_data, batch_size=aug_batch_size, shuffle=True)\nvalid_loader = DataLoader(valid_data, batch_size=64, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-07-25T11:03:11.213778Z","iopub.execute_input":"2024-07-25T11:03:11.214744Z","iopub.status.idle":"2024-07-25T11:03:11.220598Z","shell.execute_reply.started":"2024-07-25T11:03:11.214709Z","shell.execute_reply":"2024-07-25T11:03:11.219491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Dataloaders: {train_loader, valid_loader}\")\nprint(f\"Length of train dataloader: {len(train_loader)} batches of {aug_batch_size}\")\nprint(f\"Length of valid dataloader: {len(valid_loader)} batches of {64}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-25T11:03:11.514387Z","iopub.execute_input":"2024-07-25T11:03:11.514768Z","iopub.status.idle":"2024-07-25T11:03:11.520617Z","shell.execute_reply.started":"2024-07-25T11:03:11.514740Z","shell.execute_reply":"2024-07-25T11:03:11.519550Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"aug_model = IMAGEModelV0(X_height=height,\n                         X_width=width,\n                         fc_out=11).to(device)","metadata":{"execution":{"iopub.status.busy":"2024-07-25T11:03:12.444639Z","iopub.execute_input":"2024-07-25T11:03:12.445307Z","iopub.status.idle":"2024-07-25T11:03:12.591841Z","shell.execute_reply.started":"2024-07-25T11:03:12.445275Z","shell.execute_reply":"2024-07-25T11:03:12.591021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Training**","metadata":{}},{"cell_type":"code","source":"loss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(aug_model.parameters(), lr=learning_rate, weight_decay=weight_decay)","metadata":{"execution":{"iopub.status.busy":"2024-07-25T11:03:13.717728Z","iopub.execute_input":"2024-07-25T11:03:13.718100Z","iopub.status.idle":"2024-07-25T11:03:13.723645Z","shell.execute_reply.started":"2024-07-25T11:03:13.718072Z","shell.execute_reply":"2024-07-25T11:03:13.722661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def aug_train(model, num_epochs, train_dl, valid_dl, loss_fn, optimizer, seed, device):\n    train_time_start = timer()\n    same_seeds(seed)\n    loss_hist_train = [0] * num_epochs\n    accuracy_hist_train = [0] * num_epochs\n    \n    loss_hist_valid = [0] * num_epochs\n    accuracy_hist_valid = [0] * num_epochs\n    \n    best_acc = 0.0\n    \n    for epoch in range(num_epochs):\n        model.train()\n        for x_batch, y_batch in tqdm(train_dl):\n            \n            x_batch, y_batch = x_batch.view(-1, 3, height, width).to(device), torch.repeat_interleave(y_batch, repeats=6).to(device)\n            pred = model(x_batch)\n            \n            loss = loss_fn(pred, y_batch)\n            loss.backward()\n            \n            optimizer.step()\n            optimizer.zero_grad()\n            \n            loss_hist_train[epoch] += loss.item()\n            is_correct = (torch.argmax(pred, dim=1) == y_batch).float()\n            accuracy_hist_train[epoch] += is_correct.sum().item()\n            \n        loss_hist_train[epoch] /= (len(train_dl.dataset)*6)\n        accuracy_hist_train[epoch] /= (len(train_dl.dataset)*6)\n        \n        \n        model.eval()\n        with torch.no_grad():\n            for x_batch, y_batch in tqdm(valid_dl):\n                x_batch, y_batch = x_batch.to(device), y_batch.squeeze(1).to(device)\n                pred = model(x_batch)\n                loss = loss_fn(pred, y_batch)\n                \n                loss_hist_valid[epoch] += loss.item()\n                is_correct = (torch.argmax(pred, dim=1) == y_batch).float()\n                accuracy_hist_valid[epoch] += is_correct.sum().item()\n                \n        loss_hist_valid[epoch] /= len(valid_dl.dataset)\n        accuracy_hist_valid[epoch] /= len(valid_dl.dataset)    \n        \n        \n        print(f\"Epoch {epoch+1}/{num_epochs}\")\n        print(\"-\"*10)\n        print(f\"Train_loss: {loss_hist_train[epoch]:.4f} | Train_accuracy: {accuracy_hist_train[epoch]*100:.2f}%\")\n        print(f\"val_loss: {loss_hist_valid[epoch]:.4f} | val_accuracy: {accuracy_hist_valid[epoch]*100:.2f}%\\n\")\n        \n        if accuracy_hist_valid[epoch] > best_acc:\n            best_acc = accuracy_hist_valid[epoch]\n            torch.save(model.state_dict(), model_path)\n            print(f'Saving model with acc {best_acc:.5f}')\n    \n    train_time_end = timer()\n    total_train_time_model_1 = print_train_time(start=train_time_start, end=train_time_end, device=device)\n    \n    return loss_hist_train, loss_hist_valid, accuracy_hist_train, accuracy_hist_valid","metadata":{"execution":{"iopub.status.busy":"2024-07-25T11:03:15.031141Z","iopub.execute_input":"2024-07-25T11:03:15.031991Z","iopub.status.idle":"2024-07-25T11:03:15.046780Z","shell.execute_reply.started":"2024-07-25T11:03:15.031955Z","shell.execute_reply":"2024-07-25T11:03:15.045790Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss_hist_train, loss_hist_valid, accuracy_hist_train, accuracy_hist_valid = aug_train(model=aug_model,\n                                                                                      num_epochs=num_epoch,\n                                                                                      train_dl=train_loader,\n                                                                                      valid_dl=valid_loader,\n                                                                                      loss_fn=loss_fn,\n                                                                                      optimizer=optimizer,\n                                                                                     seed=seed, device=device)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-25T11:03:15.912581Z","iopub.execute_input":"2024-07-25T11:03:15.913239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Pre-define CNN Model","metadata":{}},{"cell_type":"markdown","source":"## Resnet","metadata":{}},{"cell_type":"code","source":"aug_transform = transforms.Compose([\n    transforms.Resize((image_size, image_size)),\n    transforms.RandomRotation(20),\n    transforms.RandomHorizontalFlip(),\n    transforms.ColorJitter(0.1, 0.1, 0.1),\n    transforms.ToTensor(),\n    transforms.Normalize(mean, std),\n    transforms.RandomErasing()\n])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = AugmentImageDataset(root_path=train_path, transform=aug_transform , label=True)\nvalid_data = ImageDataset(root_path=valid_path, transform=test_transform, label=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\nvalid_loader = DataLoader(valid_data, batch_size=64, shuffle=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = models.resnet34(weights='IMAGENET1K_V1')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"in_features = int(model.fc.in_features)\nmodel.fc = nn.Linear(in_features, 11, device)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"resnet_model = model.to(device)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\nlr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss_hist_train, loss_hist_valid, accuracy_hist_train, accuracy_hist_valid = aug_train(model=resnet_model,\n                                                                                      num_epochs=num_epoch,\n                                                                                      train_dl=train_loader,\n                                                                                      valid_dl=valid_loader,\n                                                                                      loss_fn=loss_fn,\n                                                                                      optimizer=optimizer,\n                                                                                     seed=seed, device=device)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# T-SNE","metadata":{}},{"cell_type":"markdown","source":"**0-2(mid) layers**","metadata":{}},{"cell_type":"code","source":"# Extract the representations for the specific layer of model\nindex = 2 # You should find out the index of layer which is defined as \"top\" or 'mid' layer of your model.\nfeatures = []\nlabels = []\nfor batch in tqdm(valid_loader):\n    imgs, lbls = batch\n    with torch.no_grad():\n        logits = model.features[:index](imgs.to(device))\n        logits = logits.view(logits.size()[0], -1)\n    labels.extend(lbls.cpu().numpy())\n    logits = np.squeeze(logits.cpu().numpy())\n    features.extend(logits)\n    \nfeatures = np.array(features)\ncolors_per_class = cm.rainbow(np.linspace(0, 1, 11))\n\n# Apply t-SNE to the features\nfeatures_tsne = TSNE(n_components=2, init='pca', random_state=42).fit_transform(features)\n\n# Plot the t-SNE visualization\nplt.figure(figsize=(10, 8))\nfor label in np.unique(labels):\n    plt.scatter(features_tsne[labels == label, 0], features_tsne[labels == label, 1], label=label, s=5)\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**0-4(top) layers**","metadata":{}},{"cell_type":"code","source":"# Extract the representations for the specific layer of model\nindex = 4 # You should find out the index of layer which is defined as \"top\" or 'mid' layer of your model.\nfeatures = []\nlabels = []\nfor batch in tqdm(valid_loader):\n    imgs, lbls = batch\n    with torch.no_grad():\n        logits = model.features[:index](imgs.to(device))\n        logits = logits.view(logits.size()[0], -1)\n    labels.extend(lbls.cpu().numpy())\n    logits = np.squeeze(logits.cpu().numpy())\n    features.extend(logits)\n    \nfeatures = np.array(features)\ncolors_per_class = cm.rainbow(np.linspace(0, 1, 11))\n\n# Apply t-SNE to the features\nfeatures_tsne = TSNE(n_components=2, init='pca', random_state=42).fit_transform(features)\n\n# Plot the t-SNE visualization\nplt.figure(figsize=(10, 8))\nfor label in np.unique(labels):\n    plt.scatter(features_tsne[labels == label, 0], features_tsne[labels == label, 1], label=label, s=5)\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prediction","metadata":{}},{"cell_type":"code","source":"with torch.no_grad():\n    for x_batch, y_batch in test_loader:\n        x_batch, y_batch = x_batch.to(device), y_batch.squeeze(1).to(device)\n        pred = model(x_batch)","metadata":{"execution":{"iopub.status.busy":"2024-07-23T12:21:03.315555Z","iopub.execute_input":"2024-07-23T12:21:03.316008Z","iopub.status.idle":"2024-07-23T12:21:03.324145Z","shell.execute_reply.started":"2024-07-23T12:21:03.315977Z","shell.execute_reply":"2024-07-23T12:21:03.322740Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}